<html>

<head>

<title>Stats: Bootstrap (January 26, 2000)</title>

</head>

<body><!--start-->

<p><strong>Bootstrap (January 27, 2000)</strong></p>

<p>This page is moving to a <a href="http://new.pmean.com/what-is-bootstrap/">new website</a>.</p>

<p><em>Dear Professor Mean, I've heard a lot about how the bootstrap is going to 
</em></p>

  <p><font face="Times New Roman">Sounds like a classic case of hype to me. 
   These are the same people who told us that computerization would lead to the 
   paperless office.</font></p>
  <p><font face="Times New Roman"><strong>Still, the bootstrap is useful for 
   estimating bias and variation</strong>. It can be applied to a surprisingly 
   wide range of problems.</font></p>

<p><strong><font face="Times New Roman">Short explanation</font></strong></p>

  <p><font face="Times New Roman">The bootstrap involves <strong>repeated 
   re-estimation of a parameter using random samples with replacement from the 
   original data</strong>. Because the sampling is with replacement, some items 
   in the data set are selected two or more times and other are not selected at 
   all. When this is repeated a hundred or a thousand times, we get 
   pseudo-samples that behave similarly to the underlying distribution of the 
   data.</font></p>
  <p><font face="Times New Roman">You can use these pseudo-samples in several 
   ways. First, you can estimate the mean of these pseudo-samples. They should 
   be close to the estimate itself. If there is a discrepancy, you have a
   <strong>quantitative estimate of bias</strong>.</font></p>
  <p><font face="Times New Roman">Second, you can look at the standard deviation 
   of these pseudo-samples. This gives you a <strong>bootstrap standard error</strong> 
   of your estimate. This standard error is not reliant on any distributional 
   assumptions (like normality).</font></p>
  <p><font face="Times New Roman">Third, you can compute the 2.5 percentile and 
   the 97.5 percentile of these pseudo-samples. This gives you a <strong>
   bootstrap confidence interval</strong>. You could also use the classic 
   formula for the confidence interval (plus or minus 1.96 standard errors).</font></p>
  <p><font face="Times New Roman">This gives you a general idea of what the 
   bootstrap can do, but you should consult with a professional statistician 
   before trying to use a bootstrap yourself. There are a lot of subtle 
   variations in how to perform the bootstrap and how to summarize results from 
   your pseudo samples.</font></p>

<p><strong><font face="Times New Roman">Example</font></strong></p>

  <p><font face="Times New Roman">The bootstrap would work like this. Suppose we 
   had a sample of four data points: 1, 3, 5, 9 and we estimated the median from 
   this sample as 4. Now we want to estimate how precise the median of four 
   numbers would be. we repeatedly sample with replacement from the four data 
   points to get the following results:</font></p>

    <p><font face="Times New Roman">Median(3,5,1,1)=2<br>
     Median(3,9,9,1)=6<br>
     Median(5,9,9,9)=9<br>
     Median(5,1,9,5)=5<br>
     Median(3,9,9,5)=7<br>
     Median(1,3,9,5)=4<br>
     Median(3,3,9,3)=3<br>
     Median(9,5,3,3)=4<br>
     Median(1,5,3,1)=2<br>
     Median(9,3,3,9)=6</font></p>

  <p><font face="Times New Roman">We have a pseudo-sample of 10 re-estimated 
   medians {2, 6, 9, 5, 7, 4, 3, 4, 2, 6}. The behavior of this pseudo-sample 
   mimics the behavior of the median. For example, the standard deviation of 
   these 10 values (2.3) is an estimate of the variability of the median.</font></p>
  <p><font face="Times New Roman">This is only 10 bootstrap samples. Normally 
   you would run at least 100, and if you wanted confidence intervals, at least 
   1000 bootstrap samples.</font></p>
  <p><font face="Times New Roman">A quick warning: there are some limitations to 
   the bootstrap when you are using a small sample (as we are here) and when you 
   are estimating certain statistics like the median. I chose this example 
   mostly because it was easy to calculate and show the results.</font></p>

<p><strong>Summary</strong></p>

  <p>Inquisitive Ian has heard a lot about <strong>how the bootstrap will 
   revolutionize statistics</strong>. Professor Mean explains that although 
   there may be some hype in that statement, the bootstrap is a <strong>flexible 
   approach for estimating bias and variability</strong> that works for a 
   surprisingly wide range of problems. The bootstrap is a <strong>simulation 
   where your computer repeatedly samples with replacement from the data set 
   itself</strong>.</p>

<p><b>How to program the bootstrap</b></p>

  <p>There is no easy way to run a bootstap in SPSS, though some of the 
   procedures in SPSS have bootstrapping built in. S-plus has a bootstrap() 
   function that makes it very easy to run a bootstrap. R has a function, 
   sample() that you could use to program a bootstrap.</p>
  <p>Here's how you would do this in S-plus</p>
  <p><code>x &lt;- c(1,3,5,9)<br>
   bootstrap.medians &lt;- bootstrap(x,median)<br>
   hist(bootstrap.medians$replicates,breaks=0.5:9.5)</code></p>
  <p><img border="0" src="../03/images/bootstrap.gif" width="192" height="143"></p>
  <p>and here is how you would do it in R.</p>

<p><strong>Further reading</strong></p>

  <p>The classic reference for the bootstrap is Bradley Efron's book. Pat Kelly 
   has a paper on the web that discusses the bootstrap and other simulation 
   methods.</p>
  <ol>
    <li><b>The Jackknife, the Bootstrap and Other Resampling Plans</b>. Efron B 
    (1982) Philadelphia: The Society for Industrial and Applied Mathematics. 
    (ISBN: 0-89871-179-7).</li>
    <li><a href="../12a/website/ComputerIntensive.asp">Interesting website: Overview 
    of Computer Intensive Statistical Inference Procedures (August 31, 2007)</a></li>
  </ol>

</body>

</html>
