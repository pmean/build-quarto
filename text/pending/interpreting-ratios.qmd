---
title: "Interpreting ratios after a log transformation"
author: "Steve Simon"
source: new
date: "2025-11-07"
categories:
- Blog post
tags:
- Modeling issues
format: 
  html:
    embed-resources: true
editor: source
execute: 
  error: true
  echo: false
page_update: complete
---

I was helping a student team doing an analysis of factors affecting length of stay. This is a highly skewed variable and shows the advantage of using a log transformation. I am reproducing the analysis here using an artificial dataset that very loosely approximates the actual data. This page will help with how to identify skewness, how to fit a model after a log transformation, and how to interpret the results clearly.

<!---more--->

## Load libraries

```{r}
#| label: setup
#| message: false
#| warning: false

library(broom)
library(tidyverse)
```

## Generate synthetic data

I am not going to show the code for generating an artificial dataset that behaves like the real data set. It is tedious and boring. Do take a look at the variables. In addition to los (length of stay measured in hours), I generated a set of ages for the patients, genders (1 for male, 0 for female), and acute status (1 for acute, 0 for non-acute).

I already calculated the log (base 2) for los.

```{r}
#| label: generate-data
#| echo: false

# Generating artificial data that 
# closely mirrors the original data
# is a bit  tricky. The mean and 
# standard deviation after a log base 2
# transformation are # 5.7 and 2.6. So
# compute a normal distribution based 
# on these, add in the effect of a few 
# covariates and then back transform to
# the original scale.

# Age has a slight but statistically 
# significant affect on log_los with a 
# slope on the log scale of 0.008. Age
# has a mean of 55 and a standard 
# deviation of 18. Gender has a not 
# statistically significant affect on
# log_los with males having a mean 
# which is 0.4 unit higher. Males 
# account for about 50% of the data.

# Acute status has a large and 
# statistically significant impact on
# log_los with non-acute cases being 
# smaller on average by 1.3 log units.
# Acute cases account for 95% of the
# cases.

n <- 200
data.frame(log_los=rnorm(n, mean=5.7, sd=2.6)) |>
  mutate(age=runif(n, min=20, max=90)) |>
  mutate(age=trunc(age)) |>
  mutate(log_los=log_los+(age-55)*0.008) |>
  mutate(gender=rbinom(n, 1, 0.5)) |>
  mutate(log_los=log_los+(gender-0.5)*0.4) |>
  mutate(acute=rbinom(n, 1, 0.80)) |>
  mutate(log_los=log_los+(acute-0.8)*1.3) |>
	mutate(log_los=pmax(log_los, abs(log_los))) |>
  mutate(los=2^log_los) |>
  mutate(los=trunc(los)) |>
	mutate(log_los=log2(los)) |>
  relocate(log_los, .after=los) -> d5
glimpse(d5)
```

## Descriptive statistics and graphs for los

Looking at some descriptive statistics and graphs for los, you will see how extremely skewed this variable is.

```{r}
#| label: descriptives-los

d5 |>
	summarize(
		los_mean=mean(los),
		los_median=median(los),
		los_sd=sd(los),
		los_min=min(los),
		los_max=max(los),
		n=n()) -> untransformed_statistics

untransformed_statistics
```

Notice the range, from 1 hour to `r max(d5$los)` hours which is roughly `r round(max(d5$los)/365, 1)` years. Also notice how much difference there is between the mean of the data and the median.

```{r}
#| label: los histogram

cc0 <- "Steve Simon, CC0"
d5 |>
  ggplot() +
  aes(los) +
	geom_histogram(
	  binwidth=7*24, 
	  color="black", 
	  fill="white") +
  labs(caption=cc0)
```

The histogram shows that most of the los values are small, with a few very large and extreme outliers on the high end.

```{r}
#| label: los boxplot
#| fig.width: 8
#| fig-height: 1.5

d5 |>
  ggplot() +
  aes(x=los, y=" ") +
	geom_boxplot() +
  labs(
    x="Length of stay in hours",
    y=" ",
    caption=cc0)
```

The boxplot also shows how bad the skewness is and how extreme the outliers are. 

## Try a log transformation

The log function will often fix a distribution that is skewed to the right and reduce the impact of outliers on the high end of the scale. It squeezes the large values together. The larger the value, the more the squeezing. Values on the low end will be stretched out much less and in some cases will be squeezed together. The net effect of this is to reduce the right skewness and bring the high end outliers closer to the rest of the data.

## Time units from one hour to one year

```{r}
#| label: squeeze-1
#| fig.width: 8
#| fig-height: 1.5

data.frame(
  x=c(1, 24, 7*24, 30*24, 365*24),
  labs=paste(1, c("hour", "day", "week", "month", "year")),
  y=1) |>
  mutate(log_x=log2(x)) -> time_units

time_units |>
  ggplot() +
  aes(x=x, y=y, label=labs) +
  geom_text(angle=90) + 
  theme(axis.text.x = element_text(angle = 90)) +
  scale_x_continuous(
    breaks=time_units$x,
    minor_breaks = NULL) +
  scale_y_continuous(
    name=" ",
    breaks=1,
    labels=" ",
    limits = c(0,2))
```

This plot shows time across several units of measurement: one hour, one day (24 hours), one week (7\*24=168 hours), one month (30\*24=720 hours), and one year (365\*24=8,760 hours). Notice how tightly packed one hour, one day, and one week are and how spread out one month and one year are.

## Time units after a log transformation

```{r}
#| label: squeeze-2
#| fig.width: 8
#| fig-height: 1.5

time_units |>
  ggplot() +
  aes(x=log_x, y=y, label=labs) +
  geom_text(angle=90) +
  theme(axis.text.x = element_text(angle = 90)) +
  scale_x_continuous(
    breaks=round(time_units$log_x, 1),
    limits=c(-1,log2(2*24*365))) +
  scale_y_continuous(
    name=" ",
    breaks=1,
    labels=" ",
    limits = c(0,2))

```

After a log transformation, one hour becomes zero, 24 hours becomes 4.6 log units, 168 hours becomes 7.4 log units, 729 hours becomes 9.5 log units, and 8,760 hours becomes 13.1 units. Notice how one hour, one day, and one week are stretched apart and how one month and one year are squeezed together.

## Interpreting log units

You don't normally think in terms of log units. There are a few exceptions. Noise as measured in decibels and earthquake intensity as measured on the Richter scale are on a log scale. But you don't think of a week long vacation as 7.4 log units.

If you do want to interpret logarithms, you can interpret them in relative units. When your hair dryer is blowing away at 80 decibels, that's loud but your 90 decibel lawn mower is ten times as loud. The San Francisco earthquake of 1906 was devastating, having been estimated at about 7.9 on the Richter scale. The more recent Loma Prieta earthquake of 1989, the one that disrupted Game 3 of the World Series of that year, was still serious. But at 6.9 on the Richter scale, it released only one-tenth of the energy of the 1906 quake.

Differences on the log scale are equivalent to ratios on the original scale of measurement. This is because of the well-known formula

$log\big(\frac{a}{b}\big)=log(a)-log(b)$

This holds true for base 2 logarithms, or logarithms using any other base.

## Interpreting a one unit shift on the log scale

For a base 2 logarithm, moving one unit to the left represents a halving on the original scale. Moving one unit to the right represent a doubling on the original scale.

```{r}
#| label: function
#| fig.width: 8
#| fig-height: 1.5

display_halving <- function(multiplier, unit) {
  data.frame(
    x=multiplier*c(0.5, 1, 2),
    labs=paste0(c("1/2 ", "1 ", "2 "), unit, c("", "", "s")),
    y=1) |>
    mutate(log_x=log2(x)) -> time_units

  time_units |>
    ggplot() +
    aes(x=log_x, y=y, label=labs) +
    geom_text(angle=90) + 
    theme(axis.text.x = element_text(angle = 90)) +
    scale_x_continuous(
      name=" ",
      breaks=round(time_units$log_x, 1),
      limits=c(-1, log2(2*365*24)),
      minor_breaks = NULL) +
    scale_y_continuous(
      name=" ",
      breaks=1,
      labels=" ",
      limits = c(0,2))
}
```

It's the same when you are looking at hours,

```{r}
#| label: half-hour
#| fig.width: 8
#| fig-height: 1.5

display_halving(1, "hour")
```

days,

```{r}
#| label: half-day
#| fig.width: 8
#| fig-height: 1.5

display_halving(24, "day")
```

weeks,

```{r}
#| label: half-week
#| fig.width: 8
#| fig-height: 1.5

display_halving(7*24, "week")
```

months,

```{r}
#| label: half-month
#| fig.width: 8
#| fig-height: 1.5

display_halving(30*24, "month")
```

or years.

```{r}
#| label: half-year
#| fig.width: 8
#| fig-height: 1.5

display_halving(365*24, "year")
```

There is a similar property for base 10 logarithms, but for the base ten logarithm, a one unit shift represents a ten-fold change.

## The log transformation doesn't help every time

It won't help for a distribution that is skewed to the left and/or has outliers on the low end. The stretching and squeezing in this case will actually make a bad situation worse.

## Applying the log transformation to los

There are no guarantees in Statistics, but the log transformation seems like a good choice for the length of stay data. The data is very very highly skewed and there are quite a few extreme outliers on the high end.

Here are some descriptive statistics.

```{r}
#| label: descriptives-log-los

d5 |>
	summarize(
		log_los_mean=mean(log_los),
		log_los_median=median(log_los),
		log_los_sd=sd(log_los),
		log_los_min=min(log_los),
		log_los_max=max(log_los),
		n=n()) -> log_statistics

log_statistics
```

Notice that there is no longer a disparity between the mean and the median. The maximum value is no longer so far away from the mean/median.

Let's look at the histogram.

```{r}
#| label: log_los histogram

d5 |>
  ggplot() +
  aes(log_los) +
	geom_histogram(
	  binwidth=1, 
	  color="black", 
	  fill="white") +
  labs(
    x="Log transformation of length of stay",
    caption=cc0)
```

This is much closer to a reasonable distribution. It may not be a perfectly bell shaped normal distribution, but it is close.

```{r}
#| label: log-los boxplot
#| fig.width: 8
#| fig-height: 1.5

d5 |>
  ggplot() +
  aes(x=log_los, y=" ") +
	geom_boxplot() +
  labs(
    x="Log transformation of length of stay",
    y=" ",
    caption=cc0) -> log_los_boxplot
log_los_boxplot
```

Compare this to the boxplot shown earlier. It is much easier to visualize what is going on.

I kept the log units for these graphs, but it is easy to replace them with units on the original scale. Here is a boxplot with units on the original scale.

```{r}
#| label: rescale
#| fig.width: 8
#| fig-height: 1.5

los_values <- c(1, 24, 7*24, 30*24, 365*24)
los_labels <- c(
    "1 hour", 
    "1 day", 
    "1 week", 
    "1 month", 
    "1 year")

log_los_boxplot +
  scale_x_continuous(
    breaks=log2(los_values),
    labels=los_labels,
    minor_breaks=NULL)
```

## Back transforming the mean

```{r}
#| label: back-transform

out <- NULL
out[1] <- log_statistics$log_los_mean
out[2] <- 2^out[1]
out[3] <- untransformed_statistics$los_mean
out[4] <- untransformed_statistics$los_median
out <- round(out, 1)
```

You saw earlier that the mean for the log length of stay was `r out[1]`.  Since this used the base 2 logarithm, you can transform back to the original scale of measurement by raising 2 to the log mean power. This produces `r out[2]` on the original scale. 

This quantity does not match the arithmetic mean `r out[3]` that you computed before the transformation.

## The geometric mean

An average on a log scale that is back-transformed to the original scale is a geometric mean. The formula for a geometric mean is

$\big(\Pi\ X_i\big)^{1/n}$

where $\Pi$ is mathematical notation for multiplication. You simply multiply all n observations together and take the result raised to the 1/n power. As a simple example, to get the geometric mean of 4 and 16, multiply the numbers together to get 64 and then take the square root of the result to get 8.

This is a technical point that is worth remembering. While researchers will often talk about geometric means while dropping the first half of the term, try to remember the distinction. It sometimes helps to describe the traditional mean

$\frac{1}{n}\Sigma X_i$

as the **arithmetic** mean when distinguishing it from the geometric mean.

It is also worth noting in our example that the geometric mean, `r out[2]`, is much closer to the median, `r out[4]` than the arithmetic mean, `r out[3]`, is. The geometric mean reduces the impact of outliers on the high end.

## Comparison of means after a log transformation

One of the variables that has a strong influence on los is acute, which is equal to zero for non-acute cases and one for acute cases. Here are descriptive statistics in each group, the mean for the base 2 log of los and the back-transformed geometric means. Acute cases have a much higher los on average.

```{r}
#| label: group-means

d5 |>
  group_by(acute) |>
  summarize(
    n=n(),
    log_los_mean=mean(log_los)) |>
  mutate(los_geometric_mean=2^log_los_mean) -> group_means

group_means
```

## Interpreting ratios

Typically, the interpretation of a ratio is done by noting how much difference there is between the estimated ratio and the value of 1. This difference is then converted to a percentage. So a ratio of 1.05 represents a 5% increase and a ratio of 0.6 represents a 40% decrease.

it gets a bit tricky for ratios of 2 or greater. A ratio of 2 represents a 100% increase, but it is better characterized as "twice as large" or a "doubling". You can also describe the ratio as a "fold change". So a ratio of 3.1 represents "more than tripling" or a "3.1 fold increase".

## Interpreting the los ratio

As noted earlier, the computed ratio for los is `r out[4]`. This could be interpreted as follows: "The average los in non-acute cases is `r out[7]`\% smaller than the average los in acute cases." with the implicit understanding that "average" here represents a geometric mean.

## The t-test after a log transformation

You can run a t-test on the log transformed los values, and it probably is preferable to using a t-test without the log transformation. Here is the output from a t-test on log los.

```{r}
#| label: log-t-test

m1 <- t.test(log_los ~ acute, data=d5)
m1

out <- NULL
out[1] <- m1$estimate[1]
out[2] <- m1$estimate[2]
out[3] <- out[1] - out[2]
out[4] <- 2^out[3]
out[5] <- group_means$los_geometric_mean[1]
out[6] <- group_means$los_geometric_mean[2]
out[7] <- 100*(1 - out[4])
out[8] <- m1$conf.int[1]
out[9] <- m1$conf.int[2]
out[10] <- 2^out[7]
out[11] <- 2^out[8]
out <- round(out, c(rep(2, 6), 0, rep(2, 4)))
```

The two means, `r out[1]` and `r out[2]` are the means on the log scale that we saw earlier. The difference between these two means is `r out[3]` on the log scale. If you back-transform this to the original scale you get `r out[4]` which is the ratio of the two geometric means, `r out[5]` and `r out[6]`.

## Back-transforming the confidence interval

The 95% confidence interval, `r out[8]` to `r out[9]` is on the log scale. If you back-transform this to the original scale, you get `r out[10]` to `r out[11]` which is a 95% confidence interval for the ratio of geometric means.