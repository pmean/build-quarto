---
title: "New quarto program"
author: "Steve Simon"
source: new
date: "2025-09-25"
categories:
- Blog post
tags:
- Linear regression
format: 
  html:
    embed-resources: true
editor: source
execute: 
  error: false
  echo: false
page_update: complete
---

The analysis of variance table is used for an analysis of variance model, but it is also used for a linear regression model. I want to offer some intuition on what all the entries mean in this table using a simple artificial dataset.

<!---more--->

```{r}
#| label: setup
#| message: false
#| warning: false

library(glue)
library(gt)
library(tidyverse)
```


```{r}
#| label: create

x <- c( 5,  7,  9, 11, 13, 15) - 1
y <- c(29, 21, 15, 25, 11, 19) + 1
y <- c(33, 19,  9, 31,  5, 23) + 1
fn <- "https://raw.githubusercontent.com/pmean/data/refs/heads/main/files/hypothetical-regression.csv"
read_csv(
  fn, 
  col_names=TRUE,
  col_types="nn") |>
  mutate(predicted=30-x) |>
  mutate(flat=mean(y)) -> hypothetical
```

## The artificial dataset

```{r}
#| label: print

hypothetical |> select(x, y)
```

Most of the time, I try to avoid using artificial datasets. I am making an exception here because I want to show the internal calculations using numbers that are easy to work with.

There are two variables with generic names, x and y. Treat x as the independent variable and y as the dependent variable.

## Descriptive statistics

```{r}
#| label: descriptive

hypothetical |>
  summarize(
    x_mean=mean(x),
    x_stdev=sd(x))
hypothetical |>
  summarize(
    y_mean=mean(y),
    y_stdev=sd(x))
```

Here are the means and standard deviations for the two variables. I would normally include an interpretation here, but for an artificial data set, no interpretation is needed.

## The analysis of variance table

-   Typically three rows
    -   Model or Regression or name of the independent variable
    -   Error or Residual
    -   Total or Corrected Total
-   Typically four columns
    -   Sum of squares or SS
    -   Degrees of freedom or df
    -   Mean square or MS
    -   F or F-ratio
    -   p or p-value

## Regression results from R

```{r}
#| label: lm

m1 <- lm(y~x, data=hypothetical)
anova(m1)
```

This is what the analysis of variance table looks like in R. The first row is the name of the independent variable, in this case, just x. The second row is Residual. The third row is...well there is no third row for total or corrected total.

The columns are in a slightly different order. The degrees of freedom, labeled "df", comes first, then the sum of squares, labeled "Sum Sq", followed by the mean square column, labeled ""Mean Sq", the F ratio, labelled "F value" and finally the p-value, labeled as "Pr(>F)".

## Analysis of variance table in SAS

![](http://pmean.com/new-images/25/anova-table-01.png)

In SAS, the first row is Model, the second is Error, and the third is Corrected Total. SAS also puts the degrees of freedom column first, labeled "DF", followed by "Sum of Squares", "Mean Square", "F Value", and "Pr>F"

## Analysis of variance table in SPSS, Linear Regression

![](http://pmean.com/new-images/25/anova-table-02.png)

SPSS provides two different analysis of variance tables. If you select Linear Regression from the menu, this is the table you get. The rows are labeled "Regression", "Residual", and "Total".

The columns are in a more logical order, in my opinion, than R or SAS. The first column, "Sum of Squares" is followed by the degrees of freedom column ("df") and the mean square column. The F-ratio, labeled just "F" and the p-value, labeled "Sig" round out the rest of the table.

## Analysis of variance table in SPSS, General Linear Model

![](http://pmean.com/new-images/25/anova-table-03.png)

If you select General Linear Model, you get six rows. You only need three, but for more complex models, you may need more. The first and third rows, "Corrected Model" and "x" are identical, but will differ in more complex regression models. SPSS includes an "Intercept" row, which you will almost always ignore. The fourth row, labeled "Error" is what you would normally see as the second row in most analysis of variance tables. The last row "Corrected Total" is what normally is placed as the third or last row in most other analysis of variance tables. The "Total" row, like the "intercept" row is another row that you almost always ignore.

The first column is labeled "Type III Sum of Squares". The "Type III" is not important in this example, but becomes important when you have multiple independent variables. The rest of the columns match the other SPSS analysis of variance table.

## Keeping only the important rows

![](http://pmean.com/new-images/25/anova-table-04.png)

It is easy to remove rows from a table in SPSS. Here is what the table would look like without rows 2, 3, and 5. This table deliberately kept the blank rows visible, but you can easily tighten things up by suppressing the blank rows entirely.

## Analysis of variance by Andy Field

-   Does not arrange sums of squares into a table
    -   Model: $SS_M$, $MS_M$
    -   Residual: $SS_R$, $MS_R$
    -   Total: $SS_T$
    
I am using a book by Andy Field in a class I am teaching, and thought it would be good to see how he lays out the analysis of variance table. Unfortunately, he does not provide a table in the chapter on linear regression. IN a later chapter on analysis of variance, he does produce such a table, but the row labels only make sense when you have defined the analysis of variance model as something different from the  is different, 

## So what, exactly is the analysis of variance table measuring

-   Model/Regression: Explained variation (signal)
-   Error/Residual: Unexplained variation (noise)
-   Total/Corrected total: Total variation

There is a relationship between the various sum of squares. The sum of squares for the model or regression represents explained variation. The sum of squares for error or residual represents unexplained variation. The two combined add up to the sum of squares total or total variation. The relative size of explained versus unexplained variation indicates whether there is a strong relationship, a weak relationship, or little or no relationship between the independent and dependent variable. Let's see what this means with a few graphs and a few calculations.

You can also think of the sum of squares for the model as the signal and the sum of squares for error as the noise. The total variation in your data is partitioned into variation associated with the signal and variation associated with noise.

## Scatterplot of the hypothetical data

```{r}
#| label: scatter

define_graph <- function(p0) {
  p0 +
    expand_limits(x=c(0, 20), y=c(0, 40)) +
    labs(
      x=" ",
      y=" ",
      caption="Simon 2025-09-25, CC0") +
    scale_y_continuous(breaks=4*(0:10)) +
    scale_x_continuous(breaks=2*(0:10))
}

hypothetical |>
  ggplot() +
  aes(x, y) -> p0

p0 |> define_graph() -> p1
p1 + geom_point()

hypothetical |>
  ggplot() +
  aes(x, y=30-x) -> q0

q0 |> define_graph() -> q1
```

In any linear regression model, even an artificial one like this, you should always display a graph. There appears to be a weak negative trend in the data.

## Least squares regression

-   Collect a sample
    -   $(X_1,Y_1),\ (X_2,Y_2),\ ...\ (X_n,Y_n)$
-   Compute residuals
    -   $e_i=Y_i-(b_0+b_1*X_i)$
    -   Choose b_0 and b_1 to minimize $\Sigma e_i^2$

The least squares principle is an approach to finding a line that is close to most of the data. It involves compromise because if you try to get too close to one datapoint, the distance to the other data points can get worse.

You measure closeness by computing the residuals. These represent a deviation between the actual data and the prediction you would get using a straight line.

Let's look at some examples.

## A bad fitting line

```{r}
#| label: functions

plot_fit <- function(p1, b0, b1) {

  predicted <- b0+b1*x
  residual <- y-predicted

  p1 +
    geom_segment(xend=x, yend=predicted) +
    geom_segment(x=0, y=b0, xend=20, yend=b0+b1*20) +
    geom_label(x=x, y=predicted, label=predicted) +
    geom_label(x=x, y=y, label=y) -> p2
  
  plot(p2)
  return(data.frame(x, y, predicted, residual))
}

display_clean_table <- function(matrix_table) {
  matrix_table |>
    data.frame() |> 
    gt() |> 
    tab_style(
      cell_text(align="center"), 
      locations=cells_body()) |> 
    tab_options(column_labels.hidden = TRUE)
}

calculate_ss <- function(y1, y2) {

  calculations_1 <- matrix("+", nrow=4, ncol=11)
  calculations_1[1, 2*(1:6)-1] <- glue("({y1}-{y2})²")
  calculations_1[2, 2*(1:6)-1] <- glue("({y1-y2})²")
  calculations_1[3, 2*(1:6)-1] <- as.character(pf$residual^2)
  calculations_1[4, 1] <- as.character(sum(pf$residual^2))
  calculations_1[4, 2:11] <- " "
  
  calculations_1 |>
    data.frame() |> 
    gt() |> 
    tab_style(
      cell_text(align="center"), 
      locations=cells_body()) |> 
    tab_options(column_labels.hidden = TRUE) -> calculations_2
  return(calculations_2)
}
```

```{r}
#| label: bad-1

pf <- plot_fit(p1, 20, 1)
```

Just to try to illustrate how linear regression works, I want to show some "trial and error" equations. The actual linear regression algorithm jumps immediately to the best equation, but let's start with a very bad guess.

This graph illustrates what a line with an intercept of 20 and a slope of 1 would look like. Notice that it tends to be too high. Most of the residuals (the difference between the observed values and the values on the fitted line) are negative.

## Sum of squared residuals for the bad fit

```{r}
#| label: bad-2

calculate_ss(pf$y, pf$predicted)
```

You measure how well (or in this case how poorly) an equation is by squaring the residuals and adding them up. Squaring does two things. First, it treats deviations above and below the line equally. Being five units above contributes a squared residual of 25 and being five units below also contributed a squared residual of 25.

Squaring the residual also tends to emphasize the larger deviations. You get a squared deviation of 100 when you are ten units above or below the line compared to a squared deviation of 16 when you are four units above or below the line.

This emphasis on large deviations means that linear regression tries very hard to avoid large deviations even for just a single data point.

Notice that the large deviations for this line lead to a very large sum of squared residuals.


## A better fitting line

```{r}
#| label: better-1

pf <- plot_fit(p1, 12, 1)
```

You can improve things somewhat by shifting the line lower. This graph shows how well a line with an intercept of 10 and a slope of 1 performs.

## Sum of squared residuals for a better fit

```{r}
#| label: better-2

calculate_ss(pf$y, pf$predicted)
```

## An even better line

```{r}
#| label: even-better-1

pf <- plot_fit(p1, 30, -1)
```

You can do even better by changing to a negative slope. The line shown here has an intercept of 30 and a slope of -1.

## Sum of squared residuals for an even better fit

```{r}
#| label: even-better-2

calculate_ss(pf$y, pf$predicted)
```

The sum of squared residuals has declined even further.

It turns out that you can't do any better than this line. If you tried to get closer to some of the data points, any gains would be offset from the points that move further away.

This is the least squares principle. The best line is one that is close to most of the data points. It minimizes the sum of squared residuals.


## SS regression plot

```{r}
#| label: ss-regression-1

y <- 30 - x
pf <- plot_fit(q1, 21, 0)
```

This plot shows the variation of the predicted values. You can think of this as a comparison of how far the predicted values for the estimated regression line deviates from a flat line. A flat line represents no signal.

If the regression line is very close to flat, this is evidence of a weak signal. A steep line (large positive or large negative slope) is evidence of a strong signal.

## SS regression calculations

```{r}
#| label: ss-regression-2

calculate_ss(pf$y, pf$predicted)
```

This shows the actual calculation of the sum of squares for the model.

You might ask. Is this value small (indicating a weak signal), or large (indicating a strong signal). To answer this question, you need to compare the variation in the predicted values or the variation in the signal to something else.

## SS regression in the analysis of variance table

```{r}
#| label: table-1

anova_table <- matrix(" ", nrow=4, ncol=6)
anova_table[1, 2:6] <- c("SS", "df", "MS", "F", "p-value")
anova_table[2:4, 1] <- c("Model", "Residual", "Total")
anova_table[2, 2] <- "70"

anova_table |>
  display_clean_table()
```

You place the sum of squares regression in the first row, first column.

## SS total plot

```{r}
#| label: ss-total-1

y <- hypothetical$y
pf <- plot_fit(p1, 21, 0)
```

To compute the sum of squares total, you compare the observed data to a flat line. These

## SS total calculations

```{r}
#| label: ss-total-2

calculate_ss(pf$y, pf$predicted)
```

There is a lot of variation in the observed values, much more than the variation in the predicted values.

## SS total in the analysis of variance table

```{r}
#| label: table-2

anova_table[4, 2] <- "646"

anova_table |>
  display_clean_table()
```

You place the sum of squares total in the third row, first column.

## Calculate R-squared

-   $R^2=\frac{SS_M}{SS_T}=\frac{70}{646}=0.108$

The sum of squares total is always larger than the sum of squares model. Sum of squares total represents variation both explained and unexplained. The ratio of sum of squares model to sum of squares total represents the proportion of explained variation. It is denoted by the symbol $R^2$.

In the hypothetical model, you look at the ratio of 70 to 646, which is 0.108. You interpret this as a weak relationship. A regression model using X to predict Y can only explain about 11% of the variation in Y.

## SS residual plot

```{r}
#| label: ss-error-1

pf <- plot_fit(p1, 30, -1)
```

The other sum of squares is the sum of squares for error or residuals. It represents the deviation of the observed values to the predicted values.

## SS residual calculations

```{r}
#| label: ss-error-2

calculate_ss(pf$y, pf$predicted)
```

## SS residual in the analysis of variance table

```{r}
#| label: table-3

anova_table[3, 2] <- "576"

anova_table |>
  display_clean_table()
```

You place the sum of squares residual in the second row, first column.

Notice how the sum of squares add up. Explained variation (SS model) plus unexplained variation (SS residual) equals total variation (SS total).

## Additive relationship among the sum of squares

$\begin{matrix}
Model & \Sigma(\hat{Y}_i-\bar{Y})^2 & \Sigma(Predicted_i-Mean)^2\ & Signal/Explained\ variation\\
Residual & \Sigma(Y_i-\hat{Y}_i)^2 & \Sigma(Observed_i-Predicted_i)^2 & Noise/Unexplained\ variation\\
Total & \Sigma(Y_i-\bar{Y})^2 & \Sigma(Observed_i-Mean)^2 & Total\ variation
\end{matrix}$

## df model

-   $df_M$ = number of independent variables

```{r}
#| label: table-4

anova_table[2, 3] <- "1"

anova_table |>
  display_clean_table()
```

The degrees of freedom represents the amount of data that is contributing to the various sum of squares quantities. The degrees of freedm is equal to the number of independent variables. In our example, there is only one independent variable, x, so the degrees of freedom associaton with the sum of squares model is 1.

## df total

-   $df_T$ = number of independent variables

```{r}
#| label: table-5

anova_table[4, 3] <- "5"

anova_table |>
  display_clean_table()
```

The degrees of freedom for the total sum of squares is equal ton the number of observations minus 1. You lose a degree of freedom because this sum of squares includes an estimate of the mean.

## df residual

-   $df_R$ = number of observations - number of independent variables - 1

```{r}
#| label: table-6

anova_table[3, 3] <- "4"

anova_table |>
  display_clean_table()
```

The degrees of freedom for the residuals or error with a loss of a degree of freedom for each independent variable and a loss of another degree of freedom for the estimated intercept.

Notice how the degrees of freedom add up.

## Mean square calculations, 1

-  $MS_M$ = $SS_M$ / $df_M$

```{r}
#| label: table-7

anova_table[2, 4] <- "70"

anova_table |>
  display_clean_table()
```

## Mean square calculations, 2

-  $MS_R$ = $SS_R$ / $df_R$

```{r}
#| label: table-8

anova_table[3, 4] <- "144"

anova_table |>
  display_clean_table()
```

## F ratio

-   F = $MS_M$ / $MS_R$
-   Tests the hypothesis
    -   $H_0:\ \beta_1=0$
    -   Accept $H_0$ if F is close to 1
    -   Reject $H_0$ if F is much larger than 1    

```{r}
#| label: table-9

anova_table[2, 5] <- "0.49"

anova_table |>
  display_clean_table()
```

## p-value

-   Tests the hypothesis
    -   $H_0:\ \beta_1=0$
    -   Accept $H_0$ if p-value is large
    -   Reject $H_0$ if p-value is small

```{r}
#| label: table-10

anova_table[2, 6] <- "0.524"

anova_table |>
  display_clean_table()
```

